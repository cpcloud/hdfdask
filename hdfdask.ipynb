{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env python\r\n",
      "# -*- coding: utf8 -*-\r\n",
      "\r\n",
      "\r\n",
      "\"\"\"\r\n",
      "Dask version of\r\n",
      "https://hdfgroup.org/wp/2015/04/putting-some-spark-into-hdf-eos/\r\n",
      "\"\"\"\r\n",
      "\r\n",
      "from __future__ import print_function, division\r\n",
      "\r\n",
      "import os\r\n",
      "import sys\r\n",
      "import glob\r\n",
      "import re\r\n",
      "import csv\r\n",
      "import shutil\r\n",
      "\r\n",
      "from itertools import chain\r\n",
      "from time import time\r\n",
      "\r\n",
      "import h5py\r\n",
      "\r\n",
      "import numpy as np\r\n",
      "\r\n",
      "import dask.bag as db\r\n",
      "\r\n",
      "from dask import do, value\r\n",
      "from dask.diagnostics import ProgressBar\r\n",
      "\r\n",
      "from cytoolz import concat\r\n",
      "\r\n",
      "from pyspark import SparkContext\r\n",
      "\r\n",
      "sc = SparkContext()\r\n",
      "\r\n",
      "rx = re.compile(r'^GSSTF_NCEP\\.3\\.(\\d{4}\\.\\d{2}\\.\\d{2})\\.he5$')\r\n",
      "\r\n",
      "\r\n",
      "def data(filename):\r\n",
      "    with h5py.File(filename, mode='r') as f:\r\n",
      "        dset = f['/HDFEOS/GRIDS/NCEP/Data Fields/Tair_2m']\r\n",
      "        fill = dset.attrs['_FillValue'][0]\r\n",
      "        x = dset[:].ravel()\r\n",
      "        cols = dset.shape[1]\r\n",
      "    cond = x != fill\r\n",
      "    nmissing = len(x) - cond.sum()\r\n",
      "    assert nmissing > 0\r\n",
      "    date = re.match(rx, os.path.basename(filename)).group(1).replace('.', '-')\r\n",
      "    return date, nmissing, x[cond], cols\r\n",
      "\r\n",
      "\r\n",
      "def summarize(date, nmissing, v, cols):\r\n",
      "    return [(date, len(v), np.mean(v), np.median(v), np.std(v, ddof=1))]\r\n",
      "\r\n",
      "\r\n",
      "def argtopk(k, x):\r\n",
      "    k = np.minimum(k, len(x))\r\n",
      "    ind = np.argpartition(x, -k)[-k:]\r\n",
      "    return ind[x[ind].argsort()]\r\n",
      "\r\n",
      "\r\n",
      "def top10(date, nmissing, v, cols):\r\n",
      "    argtop, argbottom = argtopk(10, v), argtopk(10, -v)\r\n",
      "    assert len(argtop) == len(argbottom), 'length of top and bottom not equal'\r\n",
      "    return [(date, int(p // cols), p % cols, v[p])\r\n",
      "            for p in chain(argtop, argbottom)]\r\n",
      "\r\n",
      "\r\n",
      "@do\r\n",
      "def store(data, path, header):\r\n",
      "    with open(path, mode='w') as f:\r\n",
      "        writer = csv.writer(f)\r\n",
      "        writer.writerow(header)\r\n",
      "        writer.writerows(concat(data))\r\n",
      "\r\n",
      "\r\n",
      "def bagit(files):\r\n",
      "    bag = db.from_sequence(files).map(data)\r\n",
      "    hc = store(bag.map(top10), 'csv/hotcold.csv',\r\n",
      "               header=('date', 'cat', 'row', 'col', 'temp'))\r\n",
      "    sm = store(bag.map(summarize), 'csv/summary.csv',\r\n",
      "               header=('date', 'len', 'mean', 'median', 'std'))\r\n",
      "    return hc, sm\r\n",
      "\r\n",
      "\r\n",
      "def doit(files):\r\n",
      "    datafiles = [do(data)(f) for f in files]\r\n",
      "    tens = [do(top10)(v[0], v[1], v[2], v[3]) for v in datafiles]\r\n",
      "    summaries = [do(summarize)(v[0], v[1], v[2], v[3])\r\n",
      "                 for v in datafiles]\r\n",
      "    hc = store(tens, 'csv/hotcold.csv',\r\n",
      "               header=('date', 'cat', 'row', 'col', 'temp'))\r\n",
      "    sm = store(summaries, 'csv/summary.csv',\r\n",
      "               header=('date', 'len', 'mean', 'median', 'std'))\r\n",
      "    return hc, sm\r\n",
      "\r\n",
      "\r\n",
      "def rmtrees(paths):\r\n",
      "    for p in paths:\r\n",
      "        if os.path.exists(p):\r\n",
      "            shutil.rmtree(p)\r\n",
      "\r\n",
      "\r\n",
      "def sparkit(files):\r\n",
      "    bag = sc.parallelize(files).map(data)\r\n",
      "    tensf, sparkf = 'csv/hotcold.spark.csv', 'csv/summary.spark.csv'\r\n",
      "    rmtrees([tensf, sparkf])\r\n",
      "    tens = bag.map(lambda args: ','.join(map(str, top10(*args)[0]))).saveAsTextFile(tensf)\r\n",
      "    summaries = bag.map(lambda args: ','.join(map(str, summarize(*args)[0]))).saveAsTextFile(sparkf)\r\n",
      "    return tens, summaries\r\n",
      "\r\n",
      "\r\n",
      "def timeit(f, files):\r\n",
      "    if f.__name__ == 'sparkit':\r\n",
      "        start = time()\r\n",
      "        result = f(files)\r\n",
      "        stop = time()\r\n",
      "        return result, stop - start\r\n",
      "    else:\r\n",
      "        dsk = value(f(files))\r\n",
      "        start = time()\r\n",
      "        with ProgressBar():\r\n",
      "            result = dsk.compute()\r\n",
      "        stop = time()\r\n",
      "        return result, stop - start\r\n",
      "\r\n",
      "\r\n",
      "def fileset_size(files):\r\n",
      "    def get_h5py_size(path):\r\n",
      "        with h5py.File(path, mode='r') as f:\r\n",
      "            dset = f['/HDFEOS/GRIDS/NCEP/Data Fields/Tair_2m']\r\n",
      "            return dset.size * dset.dtype.itemsize\r\n",
      "\r\n",
      "    return sum(get_h5py_size(f) for f in files)\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    files = [f for f in glob.glob(os.path.join('raw', '*.he5'))]\r\n",
      "    total_size = fileset_size(files)\r\n",
      "    runner = dict(do=doit, bag=bagit, spark=sparkit)[sys.argv[1]]\r\n",
      "    _, d = timeit(runner, files)\r\n",
      "    print('%s, %.2f G, %d files, %.2f s' %\r\n",
      "          (runner.__name__, total_size / 1e9, len(files), d))\r\n",
      "    # heatmap('csv/hotcold.csv')\r\n"
     ]
    }
   ],
   "source": [
    "!cat daskit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from itertools import chain\n",
    "from time import time\n",
    "\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import dask.bag as db\n",
    "\n",
    "from dask import do, value\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from cytoolz import concat\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "\n",
    "rx = re.compile(r'^GSSTF_NCEP\\.3\\.(\\d{4}\\.\\d{2}\\.\\d{2})\\.he5$')\n",
    "\n",
    "\n",
    "def data(filename):\n",
    "    with h5py.File(filename, mode='r') as f:\n",
    "        dset = f['/HDFEOS/GRIDS/NCEP/Data Fields/Tair_2m']\n",
    "        fill = dset.attrs['_FillValue'][0]\n",
    "        x = dset[:].ravel()\n",
    "        cols = dset.shape[1]\n",
    "    cond = x != fill\n",
    "    nmissing = len(x) - cond.sum()\n",
    "    assert nmissing > 0\n",
    "    date = re.match(rx, os.path.basename(filename)).group(1).replace('.', '-')\n",
    "    return date, nmissing, x[cond], cols\n",
    "\n",
    "\n",
    "def summarize(date, nmissing, v, cols):\n",
    "    return [(date, len(v), np.mean(v), np.median(v), np.std(v, ddof=1))]\n",
    "\n",
    "\n",
    "def argtopk(k, x):\n",
    "    k = np.minimum(k, len(x))\n",
    "    ind = np.argpartition(x, -k)[-k:]\n",
    "    return ind[x[ind].argsort()]\n",
    "\n",
    "\n",
    "def top10(date, nmissing, v, cols):\n",
    "    argtop, argbottom = argtopk(10, v), argtopk(10, -v)\n",
    "    assert len(argtop) == len(argbottom), 'length of top and bottom not equal'\n",
    "    return [(date, int(p // cols), p % cols, v[p])\n",
    "            for p in chain(argtop, argbottom)]\n",
    "\n",
    "\n",
    "@do\n",
    "def store(data, path, header):\n",
    "    with open(path, mode='w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(concat(data))\n",
    "\n",
    "\n",
    "def bagit(files):\n",
    "    bag = db.from_sequence(files).map(data)\n",
    "    hc = store(bag.map(top10), 'csv/hotcold.csv',\n",
    "               header=('date', 'cat', 'row', 'col', 'temp'))\n",
    "    sm = store(bag.map(summarize), 'csv/summary.csv',\n",
    "               header=('date', 'len', 'mean', 'median', 'std'))\n",
    "    return hc, sm\n",
    "\n",
    "\n",
    "def doit(files):\n",
    "    datafiles = [do(data)(f) for f in files]\n",
    "    tens = [do(top10)(v[0], v[1], v[2], v[3]) for v in datafiles]\n",
    "    summaries = [do(summarize)(v[0], v[1], v[2], v[3])\n",
    "                 for v in datafiles]\n",
    "    hc = store(tens, 'csv/hotcold.csv',\n",
    "               header=('date', 'cat', 'row', 'col', 'temp'))\n",
    "    sm = store(summaries, 'csv/summary.csv',\n",
    "               header=('date', 'len', 'mean', 'median', 'std'))\n",
    "    return hc, sm\n",
    "\n",
    "\n",
    "def rmtrees(paths):\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            shutil.rmtree(p)\n",
    "\n",
    "\n",
    "def sparkit(files):\n",
    "    bag = sc.parallelize(files).map(data)\n",
    "    tensf, sparkf = 'csv/hotcold.spark.csv', 'csv/summary.spark.csv'\n",
    "    rmtrees([tensf, sparkf])\n",
    "    tens = bag.map(lambda args: ','.join(map(str, top10(*args)[0]))).saveAsTextFile(tensf)\n",
    "    summaries = bag.map(lambda args: ','.join(map(str, summarize(*args)[0]))).saveAsTextFile(sparkf)\n",
    "    return tens, summaries\n",
    "\n",
    "\n",
    "def timeit(f, files):\n",
    "    if f.__name__ == 'sparkit':\n",
    "        start = time()\n",
    "        result = f(files)\n",
    "        stop = time()\n",
    "        return result, stop - start\n",
    "    else:\n",
    "        dsk = value(f(files))\n",
    "        start = time()\n",
    "        with ProgressBar():\n",
    "            result = dsk.compute()\n",
    "        stop = time()\n",
    "        return result, stop - start\n",
    "\n",
    "\n",
    "def fileset_size(files):\n",
    "    def get_h5py_size(path):\n",
    "        with h5py.File(path, mode='r') as f:\n",
    "            dset = f['/HDFEOS/GRIDS/NCEP/Data Fields/Tair_2m']\n",
    "            return dset.size * dset.dtype.itemsize\n",
    "\n",
    "    return sum(get_h5py_size(f) for f in files)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    files = [f for f in glob.glob(os.path.join('raw', '*.he5'))]\n",
    "    total_size = fileset_size(files)\n",
    "    runner = dict(do=doit, bag=bagit, spark=sparkit)[sys.argv[1]]\n",
    "    _, d = timeit(runner, files)\n",
    "    print('%s, %.2f G, %d files, %.2f s' %\n",
    "          (runner.__name__, total_size / 1e9, len(files), d))\n",
    "    # heatmap('csv/hotcold.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
